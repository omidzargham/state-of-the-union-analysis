{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An analysis of the State of the Union speeches\n",
    "\n",
    "**Authors: Paul Kim, Yizhuang Kang, Omid Zargham**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "The following paper explores and analyzes the text of US State of the Union Addresses from 1790-Present. After some basic exploratory analysis of the metadata, we use NLTK to compute general text analysis metrics, create a term-document matrix for direct corpus analysis, present scatterplots of the speeches using Multi-dimensional scaling techniques to collapse the high dimensional data onto two dimensions, and then compute a variety of complexity metrics to investigate the changes in complexity to US State of the Union Addresses over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "A common criticism of modern American politics is that it has gotten dumbed down over time. The level of discourse of our modern politics is said to pale in comparison to the discourse of such intellectual giants of the past, from Thomas Jefferson to FDR. We sought to examine whether such criticisms are valid, or are merely the results of malcontents looking at the past with rose-tinted glasses. By evaluating State of the Union speech complexity using a variety of metrics, we found that the complexity of State of the Union speeches show a strong trend toward decreasing complexity over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 1: \n",
    "To begin, we represented each speech in the dataset as a row in a Pandas dataframe, with information such as the date that we use to explore our given dataset. Using the date, we computed the numbers of speeches given in each month:\n",
    "<img src=\"fig/addresses_month.png\">\n",
    "Further exploration showed us a gap in the number of speeches, attributed to the dataset missing data for speeches made by Grover Cleveland in his second term between 1892 and 1897: \n",
    "<img src=\"fig/timeline.png\">\n",
    "Then, we represented the speeches as a list with each element being the raw text of each speech directly from the dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: \n",
    "In part 2, we used NLTK to compute certain text analysis metrics on the speeches through tokenizing the speech words, such as the number of non-unique words, unique words, non-unique stemmed words, unique stemmed words, characters, and sentences. In our analysis, we decided to include numbers as well as letters since we deemed them to be a part of the speech. However, we filtered out elements of the speech we felt were irrelevant to the analysis such as the most common english stop words, punctuation, and only kept the stem of words as dictated by the SnowballStemmer algorithm. We used this stemmer because it produced us figures that match the data given by the guideposts, and further research indicated that this stemmer algorithm is well-established for text analysis.\n",
    "\n",
    "We then created graphs to visualize the changes in these metrics over time:\n",
    "<img src=\"fig/speech_changes.png\">\n",
    "\n",
    "We also represented the above charts as violin plots by president, to help discern and partition the data as discrete elements rather than as a time function:\n",
    "\n",
    "<img src=\"fig/speech_characteristics.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: \n",
    "During part 3 of the project, we have loaded the data from part 2 and save the variables as speech_word and speeches_cleaned. In the first cell, we use for loop to go through the speeches_cleaned line-by-line to get all the unique words and set all unique words into a new variable called unique_word.  For the future use, we used sorted() function to make our unique_word variable contains the numbers of unique words for each president in order.  The results gave us 18797 unique words in total.  To create a function called word_vector(doc, vocab), we use two for loops to add each word and the number of it appears into one matrix.  For stem words in speeches_cleaned, we append the speech(number of the word appears) and the unique word into a vector and create a matrix.  Lastly, we transformed the matrix into data frame using pd.DataFrame and np.array functions, and take the transpose to make “words” as rows, and count as “columns”.  Now the wmat matrix contains the number of counts for each word in the entire document.\n",
    "For the last calculation part, we found out number of words that has 0 in wmat, and take the summation of number of zeros for each president and take summation again for each row.  This gave us the total number of zeros in the whole matrix, and divided by the total number of entries.  Total number of entries can be calculated as the number of rows multiple the number of columns.  Shape.[0] and shape.[1].\n",
    "\n",
    "Finally we got 93.15% zeros amount all the data, which indicated that a huge amount of words are not been said for different presidents.  President in each term had their own way of giving speeches, and it was not too common to see a lot of repeating in the words they said.  That is the reason more than 93% of the wmat entries are zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: \n",
    "Using the Term-Document matrix calculated in part 3, we normalize the word counts to convert the counts into probability distributions, for both the speeches grouped by president and for each speech individually. We then perform multidimensional scaling on the count probabilities via manifold learning, with a stress majorization optimization strategy, using euclidean distances and Jensen-Shannon Divergence as our difference metrics.\n",
    "\n",
    "The scatter plots of the word-document and president-aggregated word-document after being projected onto the planes of their stress majorizing axes, are as follows:\n",
    "\n",
    "<img src=\"fig/mds_naive.png\">\n",
    "\n",
    "<img src=\"fig/mds_naive_all.png\">\n",
    "\n",
    "<img src=\"fig/mds_jsdiv.png\">\n",
    "\n",
    "<img src=\"fig/mds_jdsiv_all.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: \n",
    "We used the following metrics as measures of complexity:\n",
    "1. Words Per Sentence\n",
    "2. Average Word Length\n",
    "3. Flesch-Kincaid Readability Score (details below)\n",
    "4. Flesch-Kincaid Grade Level (details below)\n",
    "5. Proportion of words in most common 7500 words\n",
    "\n",
    "The Flesch-Kincaid Readability Score is calculated by\n",
    "\n",
    "        206.835 - 1.015(words/sentence) - 84.6(syllables/word)\n",
    "        \n",
    "With the readablity scores being interpreted as follows:\n",
    "\n",
    "<img src=\"fig/fk_table.png\">\n",
    "\n",
    "The Flesch-Kincaid Grade Level is calculated by\n",
    "\n",
    "        0.39(words/sentence) + 11.8(syllables/word) - 15.59\n",
    "        \n",
    "With the output value corresponding to the reading level of the US Grade. \n",
    "\n",
    "For the proportion of words in the most common 7500 words, we used data from https://github.com/first20hours/google-10000-english, which contains the 10000 most common English language words as found by Google's Trillion Word Corpus.\n",
    "\n",
    "<img src=\"fig/words_per_sentence.png\">\n",
    "\n",
    "<img src=\"fig/Average_Word_Length.png\">\n",
    "\n",
    "<img src=\"fig/Readability_Score.png\">\n",
    "\n",
    "<img src=\"fig/Grade_Level.png\">\n",
    "\n",
    "<img src=\"fig/Percent_Most_Common.png\">\n",
    "\n",
    "<img src=\"fig/Readability_vs_PCW.png\">\n",
    "\n",
    "<img src=\"fig/WPS_vs_AWL.png\">\n",
    "\n",
    "From the figures, it is clear that over time, the complexity of the US State of the Union speeches has declines. The number of words per sentence, the average word length, and the grade level all have a decreasing trend, while the readability scores and the percent of words in the speech that are among the top 7500 most commonly used in the English language, all increase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
